#!/usr/bin/env python3
"""
Nautilus Massive Data Downloader
Download massive datasets from all 8 real data sources for extreme stress testing.
Downloads historical data, real-time feeds, and creates comprehensive datasets.
"""

import asyncio
import aiohttp
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import logging
import os
import json
import time
import sqlite3
from pathlib import Path
import yfinance as yf
import requests
import zipfile
import io
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DataSourceConfig:
    """Configuration for each data source"""
    name: str
    api_key: Optional[str]
    base_url: str
    rate_limit_per_minute: int
    data_types: List[str]
    historical_available: bool

class MassiveDataDownloader:
    """Downloads massive real datasets from all 8 Nautilus data sources"""
    
    def __init__(self):
        self.data_sources = {
            "alpha_vantage": DataSourceConfig(
                name="Alpha Vantage",
                api_key="271AHP91HVAPDRGP",  # From system config
                base_url="https://www.alphavantage.co/query",
                rate_limit_per_minute=5,  # Free tier limit
                data_types=["TIME_SERIES_DAILY", "TIME_SERIES_INTRADAY", "OVERVIEW", "EARNINGS", "BALANCE_SHEET"],
                historical_available=True
            ),
            "yahoo_finance": DataSourceConfig(
                name="Yahoo Finance",
                api_key=None,
                base_url="https://query1.finance.yahoo.com/v8/finance/chart",
                rate_limit_per_minute=100,
                data_types=["prices", "volumes", "indicators"],
                historical_available=True
            ),
            "fred": DataSourceConfig(
                name="FRED (Federal Reserve)",
                api_key="1f1ba9c949e988e12796b7c1f6cce1bf",  # From system config
                base_url="https://api.stlouisfed.org/fred",
                rate_limit_per_minute=120,
                data_types=["series", "observations", "releases"],
                historical_available=True
            ),
            "edgar": DataSourceConfig(
                name="SEC EDGAR",
                api_key=None,
                base_url="https://www.sec.gov/Archives/edgar",
                rate_limit_per_minute=10,
                data_types=["company_filings", "financial_statements", "insider_trading"],
                historical_available=True
            ),
            "data_gov": DataSourceConfig(
                name="Data.gov",
                api_key=None,
                base_url="https://catalog.data.gov/api/3",
                rate_limit_per_minute=60,
                data_types=["economic_indicators", "government_spending", "employment"],
                historical_available=True
            ),
            "trading_economics": DataSourceConfig(
                name="Trading Economics",
                api_key=None,  # Would need paid subscription
                base_url="https://api.tradingeconomics.com",
                rate_limit_per_minute=30,
                data_types=["indicators", "calendar", "forecasts"],
                historical_available=True
            ),
            "dbnomics": DataSourceConfig(
                name="DBnomics",
                api_key=None,
                base_url="https://api.db.nomics.world/v22",
                rate_limit_per_minute=100,
                data_types=["datasets", "series", "observations"],
                historical_available=True
            ),
            "ibkr": DataSourceConfig(
                name="Interactive Brokers",
                api_key=None,
                base_url="http://localhost:8800",  # Our enhanced IBKR engine
                rate_limit_per_minute=1000,
                data_types=["market_data", "level2", "trades", "historical"],
                historical_available=True
            )
        }
        
        # Symbols for massive data download
        self.major_symbols = [
            # Large Cap Tech
            "AAPL", "MSFT", "GOOGL", "GOOG", "AMZN", "META", "TSLA", "NVDA", "CRM", "ORCL",
            "IBM", "INTC", "AMD", "ADBE", "NFLX", "PYPL", "UBER", "LYFT", "TWTR", "SNAP",
            
            # Financial
            "JPM", "BAC", "WFC", "GS", "MS", "C", "USB", "PNC", "TFC", "COF",
            "AXP", "BK", "STT", "NTRS", "BLK", "SCHW", "SPGI", "MCO", "ICE", "CME",
            
            # Healthcare
            "JNJ", "UNH", "PFE", "ABT", "TMO", "DHR", "BMY", "ABBV", "MRK", "LLY",
            "GILD", "AMGN", "BIIB", "VRTX", "REGN", "ILMN", "ISRG", "SYK", "BSX", "MDT",
            
            # Energy
            "XOM", "CVX", "COP", "EOG", "SLB", "OXY", "KMI", "WMB", "MPC", "VLO",
            "PSX", "HES", "DVN", "FANG", "MRO", "APA", "OKE", "ET", "EPD", "MPLX",
            
            # Consumer
            "WMT", "HD", "PG", "KO", "PEP", "MCD", "SBUX", "NKE", "DIS", "COST",
            "TGT", "LOW", "F", "GM", "TSLA", "CCL", "RCL", "MAR", "HLT", "MGM",
            
            # Industrial
            "BA", "CAT", "GE", "MMM", "HON", "UPS", "FDX", "LMT", "RTX", "NOC",
            "GD", "DE", "EMR", "ETN", "ITW", "PH", "ROK", "DOV", "XYL", "FTV",
            
            # ETFs for broad market exposure
            "SPY", "QQQ", "IWM", "VTI", "VEA", "VWO", "AGG", "BND", "GLD", "SLV",
            "XLE", "XLF", "XLK", "XLV", "XLI", "XLY", "XLP", "XLB", "XLU", "XLRE",
            
            # Crypto ETFs and related
            "GBTC", "ETHE", "BITO", "ARKK", "ARKQ", "ARKW", "ARKG", "ARKF",
            
            # International
            "EFA", "EEM", "FXI", "EWJ", "EWZ", "INDA", "RSX", "ASHR", "MCHI", "KWEB"
        ]
        
        # Economic indicators from FRED
        self.fred_indicators = [
            "GDP", "GDPC1", "CPIAUCSL", "CPILFESL", "UNRATE", "PAYEMS", "FEDFUNDS", 
            "DGS10", "DGS2", "DGS30", "T10Y2Y", "DEXUSEU", "DEXJPUS", "DEXCHUS",
            "HOUST", "PERMIT", "RETAILSL", "INDPRO", "CAPUTLB00004S", "UMCSENT",
            "NASDAQCOM", "SP500", "VIXCLS", "TEDRATE", "T10YIE", "DFII10", "BAMLH0A0HYM2",
            "MORTGAGE30US", "REAINTRATREARAT10Y", "CSUSHPISA", "MEHOINUSA646N", "DSPIC96"
        ]
        
        self.session = None
        self.download_stats = {
            "total_symbols": 0,
            "successful_downloads": 0,
            "failed_downloads": 0,
            "total_data_points": 0,
            "start_time": None,
            "data_sources_used": 0
        }

    async def initialize(self):
        """Initialize HTTP session and create directories"""
        logger.info("ğŸš€ Initializing Massive Data Downloader...")
        
        # Create data directories
        data_dir = Path("massive_datasets")
        for source in self.data_sources.keys():
            (data_dir / source).mkdir(parents=True, exist_ok=True)
        
        timeout = aiohttp.ClientTimeout(total=60, connect=10)
        self.session = aiohttp.ClientSession(timeout=timeout)
        
        self.download_stats["start_time"] = time.time()
        logger.info("âœ… Downloader initialized")

    async def download_alpha_vantage_data(self, symbols: List[str]) -> Dict[str, Any]:
        """Download comprehensive data from Alpha Vantage"""
        logger.info(f"ğŸ“Š Downloading Alpha Vantage data for {len(symbols)} symbols...")
        
        config = self.data_sources["alpha_vantage"]
        downloaded_data = {}
        
        for symbol in symbols[:20]:  # Limit due to API restrictions
            try:
                # Daily time series (5 years)
                params = {
                    "function": "TIME_SERIES_DAILY_ADJUSTED",
                    "symbol": symbol,
                    "outputsize": "full",
                    "apikey": config.api_key
                }
                
                async with self.session.get(config.base_url, params=params) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if "Time Series (Daily)" in data:
                            downloaded_data[f"{symbol}_daily"] = data
                            self.download_stats["successful_downloads"] += 1
                            
                            # Count data points
                            time_series = data["Time Series (Daily)"]
                            self.download_stats["total_data_points"] += len(time_series)
                            
                            logger.info(f"âœ… {symbol}: Downloaded {len(time_series)} daily data points")
                        else:
                            logger.warning(f"âš ï¸ {symbol}: No time series data in response")
                            self.download_stats["failed_downloads"] += 1
                    else:
                        logger.warning(f"âš ï¸ {symbol}: HTTP {resp.status}")
                        self.download_stats["failed_downloads"] += 1
                
                # Company overview
                params = {
                    "function": "OVERVIEW",
                    "symbol": symbol,
                    "apikey": config.api_key
                }
                
                async with self.session.get(config.base_url, params=params) as resp:
                    if resp.status == 200:
                        overview = await resp.json()
                        if "Symbol" in overview:
                            downloaded_data[f"{symbol}_overview"] = overview
                            logger.info(f"âœ… {symbol}: Downloaded company overview")
                
                # Rate limiting
                await asyncio.sleep(12)  # Alpha Vantage free tier: 5 calls per minute
                
            except Exception as e:
                logger.error(f"âŒ {symbol} Alpha Vantage error: {e}")
                self.download_stats["failed_downloads"] += 1
        
        # Save to file
        with open("massive_datasets/alpha_vantage/comprehensive_data.json", "w") as f:
            json.dump(downloaded_data, f, indent=2, default=str)
        
        return {"source": "alpha_vantage", "symbols": len(downloaded_data), "data": downloaded_data}

    async def download_yahoo_finance_data(self, symbols: List[str]) -> Dict[str, Any]:
        """Download massive historical data from Yahoo Finance using yfinance"""
        logger.info(f"ğŸ“ˆ Downloading Yahoo Finance data for {len(symbols)} symbols...")
        
        downloaded_data = {}
        
        def download_symbol_data(symbol):
            try:
                ticker = yf.Ticker(symbol)
                
                # Get 5 years of historical data
                hist = ticker.history(period="5y", interval="1d")
                if not hist.empty:
                    # Get additional data
                    info = ticker.info
                    dividends = ticker.dividends
                    splits = ticker.splits
                    
                    return {
                        "symbol": symbol,
                        "historical": hist.to_dict(),
                        "info": info,
                        "dividends": dividends.to_dict() if not dividends.empty else {},
                        "splits": splits.to_dict() if not splits.empty else {},
                        "data_points": len(hist)
                    }
                else:
                    return None
            except Exception as e:
                logger.error(f"âŒ {symbol} Yahoo Finance error: {e}")
                return None

        # Use ThreadPoolExecutor for parallel downloads
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_symbol = {executor.submit(download_symbol_data, symbol): symbol 
                              for symbol in symbols}
            
            for future in as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                try:
                    result = future.result(timeout=30)
                    if result:
                        downloaded_data[symbol] = result
                        self.download_stats["successful_downloads"] += 1
                        self.download_stats["total_data_points"] += result["data_points"]
                        logger.info(f"âœ… {symbol}: {result['data_points']} data points")
                    else:
                        self.download_stats["failed_downloads"] += 1
                except Exception as e:
                    logger.error(f"âŒ {symbol}: {e}")
                    self.download_stats["failed_downloads"] += 1
        
        # Save to file
        with open("massive_datasets/yahoo_finance/historical_data.json", "w") as f:
            json.dump(downloaded_data, f, indent=2, default=str)
        
        return {"source": "yahoo_finance", "symbols": len(downloaded_data), "data": downloaded_data}

    async def download_fred_data(self, indicators: List[str]) -> Dict[str, Any]:
        """Download economic indicators from FRED"""
        logger.info(f"ğŸ¦ Downloading FRED data for {len(indicators)} indicators...")
        
        config = self.data_sources["fred"]
        downloaded_data = {}
        
        for indicator in indicators:
            try:
                # Get series observations
                params = {
                    "series_id": indicator,
                    "api_key": config.api_key,
                    "file_type": "json",
                    "limit": 10000  # Get lots of historical data
                }
                
                url = f"{config.base_url}/series/observations"
                async with self.session.get(url, params=params) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if "observations" in data:
                            observations = data["observations"]
                            downloaded_data[indicator] = observations
                            self.download_stats["successful_downloads"] += 1
                            self.download_stats["total_data_points"] += len(observations)
                            logger.info(f"âœ… {indicator}: {len(observations)} observations")
                        else:
                            logger.warning(f"âš ï¸ {indicator}: No observations in response")
                            self.download_stats["failed_downloads"] += 1
                    else:
                        logger.warning(f"âš ï¸ {indicator}: HTTP {resp.status}")
                        self.download_stats["failed_downloads"] += 1
                
                # Rate limiting
                await asyncio.sleep(0.5)  # FRED allows 120 calls per minute
                
            except Exception as e:
                logger.error(f"âŒ {indicator} FRED error: {e}")
                self.download_stats["failed_downloads"] += 1
        
        # Save to file
        with open("massive_datasets/fred/economic_indicators.json", "w") as f:
            json.dump(downloaded_data, f, indent=2, default=str)
        
        return {"source": "fred", "indicators": len(downloaded_data), "data": downloaded_data}

    async def download_ibkr_data(self) -> Dict[str, Any]:
        """Download real-time and historical data from our enhanced IBKR engine"""
        logger.info("ğŸ¢ Downloading IBKR data from enhanced engine...")
        
        config = self.data_sources["ibkr"]
        downloaded_data = {}
        
        try:
            # Get current market data from IBKR engine
            async with self.session.get(f"{config.base_url}/market_data/current") as resp:
                if resp.status == 200:
                    current_data = await resp.json()
                    downloaded_data["current_market_data"] = current_data
                    logger.info("âœ… Downloaded current IBKR market data")
            
            # Get active symbols
            async with self.session.get(f"{config.base_url}/symbols/active") as resp:
                if resp.status == 200:
                    symbols_data = await resp.json()
                    downloaded_data["active_symbols"] = symbols_data
                    logger.info(f"âœ… Downloaded {len(symbols_data.get('symbols', []))} active symbols")
            
            # Get Level 2 order book data for major symbols
            for symbol in self.major_symbols[:10]:  # Top 10 symbols
                try:
                    async with self.session.get(f"{config.base_url}/level2/{symbol}") as resp:
                        if resp.status == 200:
                            level2_data = await resp.json()
                            downloaded_data[f"{symbol}_level2"] = level2_data
                            self.download_stats["successful_downloads"] += 1
                            logger.info(f"âœ… {symbol}: Downloaded Level 2 data")
                        else:
                            self.download_stats["failed_downloads"] += 1
                except Exception as e:
                    logger.error(f"âŒ {symbol} Level 2 error: {e}")
                    self.download_stats["failed_downloads"] += 1
                
                await asyncio.sleep(0.1)  # Brief rate limiting
            
            self.download_stats["total_data_points"] += len(downloaded_data)
            
        except Exception as e:
            logger.error(f"âŒ IBKR engine error: {e}")
        
        # Save to file
        with open("massive_datasets/ibkr/market_data.json", "w") as f:
            json.dump(downloaded_data, f, indent=2, default=str)
        
        return {"source": "ibkr", "datasets": len(downloaded_data), "data": downloaded_data}

    async def download_dbnomics_data(self) -> Dict[str, Any]:
        """Download international economic data from DBnomics"""
        logger.info("ğŸŒ Downloading DBnomics international data...")
        
        config = self.data_sources["dbnomics"]
        downloaded_data = {}
        
        # Key international datasets
        datasets = [
            "BIS/WS_CBPOL_D/M.N.A",  # Central bank policy rates
            "OECD/MEI/USA.LOLITOAA_IXOB.M",  # US leading indicators
            "ECB/EXR/M.USD.EUR.SP00.A",  # EUR/USD exchange rate
            "IMF/IFS/A.US.PCPI_IX",  # US CPI
            "WORLD_BANK/WDI/USA.NY.GDP.MKTP.CD"  # US GDP
        ]
        
        for dataset in datasets:
            try:
                url = f"{config.base_url}/series/{dataset}/observations"
                params = {"limit": 1000, "format": "json"}
                
                async with self.session.get(url, params=params) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if "series" in data:
                            downloaded_data[dataset] = data
                            self.download_stats["successful_downloads"] += 1
                            obs_count = len(data.get("series", {}).get("observations", []))
                            self.download_stats["total_data_points"] += obs_count
                            logger.info(f"âœ… {dataset}: {obs_count} observations")
                        else:
                            self.download_stats["failed_downloads"] += 1
                    else:
                        logger.warning(f"âš ï¸ {dataset}: HTTP {resp.status}")
                        self.download_stats["failed_downloads"] += 1
                
                await asyncio.sleep(0.6)  # Rate limiting
                
            except Exception as e:
                logger.error(f"âŒ {dataset} DBnomics error: {e}")
                self.download_stats["failed_downloads"] += 1
        
        # Save to file
        with open("massive_datasets/dbnomics/international_data.json", "w") as f:
            json.dump(downloaded_data, f, indent=2, default=str)
        
        return {"source": "dbnomics", "datasets": len(downloaded_data), "data": downloaded_data}

    async def download_comprehensive_datasets(self) -> Dict[str, Any]:
        """Download massive datasets from all sources"""
        logger.info("ğŸŒŠ Starting massive data download from all 8 sources...")
        
        self.download_stats["total_symbols"] = len(self.major_symbols) + len(self.fred_indicators)
        
        # Download from all sources concurrently
        download_tasks = [
            self.download_alpha_vantage_data(self.major_symbols[:20]),  # API limits
            self.download_yahoo_finance_data(self.major_symbols),
            self.download_fred_data(self.fred_indicators),
            self.download_ibkr_data(),
            self.download_dbnomics_data()
        ]
        
        results = await asyncio.gather(*download_tasks, return_exceptions=True)
        
        successful_sources = []
        failed_sources = []
        
        for result in results:
            if isinstance(result, Exception):
                failed_sources.append(str(result))
                logger.error(f"âŒ Source failed: {result}")
            else:
                successful_sources.append(result)
                self.download_stats["data_sources_used"] += 1
        
        # Calculate final stats
        total_time = time.time() - self.download_stats["start_time"]
        
        comprehensive_summary = {
            "download_summary": {
                "total_time_seconds": total_time,
                "total_symbols_requested": self.download_stats["total_symbols"],
                "successful_downloads": self.download_stats["successful_downloads"],
                "failed_downloads": self.download_stats["failed_downloads"],
                "success_rate": (self.download_stats["successful_downloads"] / 
                               max(1, self.download_stats["successful_downloads"] + self.download_stats["failed_downloads"])),
                "total_data_points": self.download_stats["total_data_points"],
                "data_sources_used": self.download_stats["data_sources_used"],
                "download_rate": self.download_stats["total_data_points"] / total_time if total_time > 0 else 0
            },
            "data_sources": successful_sources,
            "failed_sources": failed_sources,
            "file_locations": {
                "alpha_vantage": "massive_datasets/alpha_vantage/comprehensive_data.json",
                "yahoo_finance": "massive_datasets/yahoo_finance/historical_data.json",
                "fred": "massive_datasets/fred/economic_indicators.json",
                "ibkr": "massive_datasets/ibkr/market_data.json",
                "dbnomics": "massive_datasets/dbnomics/international_data.json"
            }
        }
        
        # Save comprehensive summary
        with open("massive_datasets/download_summary.json", "w") as f:
            json.dump(comprehensive_summary, f, indent=2, default=str)
        
        logger.info(f"""
ğŸ‰ Massive Data Download Completed!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸  Total Time: {total_time:.1f} seconds
ğŸ“Š Data Points Downloaded: {self.download_stats['total_data_points']:,}
âœ… Successful Downloads: {self.download_stats['successful_downloads']}
âŒ Failed Downloads: {self.download_stats['failed_downloads']}
ğŸ¯ Success Rate: {comprehensive_summary['download_summary']['success_rate']:.1%}
ğŸŒ Data Sources Used: {self.download_stats['data_sources_used']}/8
ğŸ“ˆ Download Rate: {comprehensive_summary['download_summary']['download_rate']:.0f} data points/second
        """)
        
        return comprehensive_summary

    async def cleanup(self):
        """Clean up resources"""
        if self.session:
            await self.session.close()

async def main():
    """Execute massive data download"""
    downloader = MassiveDataDownloader()
    
    try:
        await downloader.initialize()
        results = await downloader.download_comprehensive_datasets()
        
        # Print final summary
        summary = results["download_summary"]
        print(f"""
ğŸ† MASSIVE DATA DOWNLOAD COMPLETE!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š DOWNLOAD STATISTICS:
â€¢ Total Data Points: {summary['total_data_points']:,}
â€¢ Success Rate: {summary['success_rate']:.1%}  
â€¢ Data Sources: {summary['data_sources_used']}/8
â€¢ Download Rate: {summary['download_rate']:.0f} points/sec
â€¢ Total Time: {summary['total_time_seconds']:.1f} seconds

ğŸ“‚ DATA FILES CREATED:
â€¢ Alpha Vantage: massive_datasets/alpha_vantage/
â€¢ Yahoo Finance: massive_datasets/yahoo_finance/
â€¢ FRED Economic: massive_datasets/fred/
â€¢ IBKR Market Data: massive_datasets/ibkr/
â€¢ DBnomics International: massive_datasets/dbnomics/

ğŸš€ Ready for extreme stress testing with real data!
        """)
        
    except Exception as e:
        logger.error(f"âŒ Download failed: {e}")
    
    finally:
        await downloader.cleanup()

if __name__ == "__main__":
    print("ğŸŒŠ Nautilus Massive Data Downloader")
    print("=" * 50)
    asyncio.run(main())