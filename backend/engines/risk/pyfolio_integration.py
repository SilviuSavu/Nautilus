#!/usr/bin/env python3
"""
PyFolio Integration Layer for Nautilus Risk Engine
=================================================

Professional portfolio analytics wrapper optimized for institutional use.
Provides comprehensive performance metrics, tear sheets, and risk attribution.

Performance Requirements:
- Basic analytics: <200ms response time
- HTML tear sheet generation: <500ms
- Memory efficient with caching capabilities
- Thread-safe operations for concurrent requests

Integration Features:
- Full PyFolio metrics compatibility
- Custom Nautilus-specific enhancements
- Async/await pattern throughout
- Comprehensive error handling
- Performance tracking and monitoring
"""

import pandas as pd
import numpy as np
from typing import Dict, Optional, Tuple, Any, List
from datetime import datetime, timedelta
import logging
import time
import io
import base64
import asyncio
from dataclasses import dataclass, asdict
from enum import Enum
import json

# ===============================================
# ENHANCED PYFOLIO INTEGRATION WITH PROPER ERROR HANDLING
# ===============================================

# Initialize availability flags
PYFOLIO_AVAILABLE = False
EMPYRICAL_AVAILABLE = False
MATPLOTLIB_AVAILABLE = False
PYFOLIO_VERSION = "Not installed"
EMPYRICAL_VERSION = "Not installed"

# PyFolio core import
try:
    import pyfolio as pf
    PYFOLIO_AVAILABLE = True
    PYFOLIO_VERSION = getattr(pf, '__version__', 'Unknown')
    logging.info(f"‚úÖ PyFolio loaded successfully - version {PYFOLIO_VERSION}")
except ImportError as e:
    logging.warning(f"‚ùå PyFolio import failed: {e}")
    logging.warning("Install PyFolio with: pip install pyfolio>=0.9.5")

# Empyrical for performance metrics
try:
    import empyrical as ep
    EMPYRICAL_AVAILABLE = True
    EMPYRICAL_VERSION = getattr(ep, '__version__', 'Unknown')
    logging.info(f"‚úÖ Empyrical loaded successfully - version {EMPYRICAL_VERSION}")
except ImportError as e:
    logging.warning(f"‚ùå Empyrical import failed: {e}")
    logging.warning("Install Empyrical with: pip install empyrical>=0.5.5")

# Matplotlib for visualization
try:
    import matplotlib
    matplotlib.use('Agg')  # Non-interactive backend for server use
    import matplotlib.pyplot as plt
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
    logging.info("‚úÖ Matplotlib loaded successfully")
except ImportError as e:
    logging.warning(f"‚ùå Matplotlib import failed: {e}")
    logging.warning("Install Matplotlib with: pip install matplotlib>=3.7.0 seaborn>=0.12.0")

# Overall integration status
PYFOLIO_FULL_INTEGRATION = PYFOLIO_AVAILABLE and EMPYRICAL_AVAILABLE and MATPLOTLIB_AVAILABLE

if PYFOLIO_FULL_INTEGRATION:
    logging.info("üéâ PyFolio full integration ready!")
else:
    missing_components = []
    if not PYFOLIO_AVAILABLE: missing_components.append("PyFolio")
    if not EMPYRICAL_AVAILABLE: missing_components.append("Empyrical") 
    if not MATPLOTLIB_AVAILABLE: missing_components.append("Matplotlib")
    logging.warning(f"‚ö†Ô∏è  PyFolio partial integration - missing: {', '.join(missing_components)}")
    logging.warning("Run 'pip install -r requirements.txt' to install missing dependencies")

logger = logging.getLogger(__name__)

class AnalyticsStatus(Enum):
    """Analytics computation status"""
    PENDING = "pending"
    COMPUTING = "computing"
    COMPLETED = "completed"
    FAILED = "failed"
    CACHED = "cached"

@dataclass
class PyFolioMetrics:
    """Structured PyFolio analytics results"""
    portfolio_id: str
    calculation_time_ms: float
    computation_date: datetime
    
    # Core performance metrics
    total_return: float
    annual_return: float
    annual_volatility: float
    sharpe_ratio: float
    max_drawdown: float
    
    # Risk metrics
    sortino_ratio: float
    calmar_ratio: float
    omega_ratio: float
    tail_ratio: float
    value_at_risk_5: float
    conditional_var_5: float
    
    # Statistical properties
    skewness: float
    kurtosis: float
    downside_deviation: float
    
    # Benchmark comparison (optional)
    alpha: Optional[float] = None
    beta: Optional[float] = None
    tracking_error: Optional[float] = None
    information_ratio: Optional[float] = None
    up_capture: Optional[float] = None
    down_capture: Optional[float] = None
    
    # Additional metrics
    status: AnalyticsStatus = AnalyticsStatus.COMPLETED

@dataclass
class TearSheetConfig:
    """Configuration for tear sheet generation"""
    include_benchmark: bool = True
    include_positions: bool = False
    include_transactions: bool = False
    rolling_window: int = 126  # 6 months
    risk_free_rate: float = 0.02
    confidence_level: float = 0.05

class PyFolioAnalytics:
    """
    Enhanced PyFolio integration for comprehensive portfolio analytics
    
    Features:
    - Institutional-grade performance metrics
    - Professional HTML tear sheet generation  
    - Async operations with performance tracking
    - Intelligent caching for repeated calculations
    - Comprehensive error handling and validation
    """
    
    def __init__(self, cache_ttl_minutes: int = 30):
        """
        Initialize PyFolio Analytics Engine
        
        Args:
            cache_ttl_minutes: Cache time-to-live in minutes
        """
        self.available = PYFOLIO_AVAILABLE
        self.version = PYFOLIO_VERSION
        
        if not self.available:
            logger.error("PyFolio not available - analytics will be limited")
        
        # Performance tracking
        self.calculations_performed = 0
        self.total_calculation_time = 0.0
        self.cache_hits = 0
        self.cache_misses = 0
        
        # Analytics cache with TTL
        self.cache_ttl = timedelta(minutes=cache_ttl_minutes)
        self.analytics_cache: Dict[str, Tuple[PyFolioMetrics, datetime]] = {}
        
        # Configuration
        self.default_config = TearSheetConfig()
        self.default_benchmark_ticker = 'SPY'
        
        logger.info(f"PyFolio Analytics initialized - Available: {self.available}, Version: {self.version}")
    
    async def compute_performance_metrics(self, 
                                        portfolio_id: str,
                                        returns: pd.Series,
                                        benchmark_returns: Optional[pd.Series] = None,
                                        config: Optional[TearSheetConfig] = None) -> PyFolioMetrics:
        """
        Compute comprehensive performance metrics using PyFolio
        
        Args:
            portfolio_id: Unique portfolio identifier
            returns: Portfolio returns time series (daily)
            benchmark_returns: Optional benchmark returns
            config: Configuration for calculations
            
        Returns:
            PyFolioMetrics object with all computed metrics
            
        Raises:
            RuntimeError: If PyFolio not available
            ValueError: If returns data invalid
        """
        if not self.available:
            raise RuntimeError("PyFolio not available - cannot compute metrics")
        
        # Validate inputs
        if returns.empty:
            raise ValueError("Returns series cannot be empty")
            
        if len(returns) < 30:  # Minimum 30 days for meaningful metrics
            raise ValueError("Insufficient data - need at least 30 days of returns")
        
        # Check cache first
        cache_key = self._generate_cache_key(portfolio_id, returns, benchmark_returns)
        cached_result = self._get_cached_analytics(cache_key)
        if cached_result:
            self.cache_hits += 1
            return cached_result
        
        self.cache_misses += 1
        config = config or self.default_config
        start_time = time.time()
        
        try:
            logger.info(f"Computing PyFolio metrics for portfolio {portfolio_id}")
            
            # Ensure returns is properly formatted
            returns = returns.dropna()
            returns.index = pd.to_datetime(returns.index)
            
            # Core performance metrics
            total_return = ep.cum_returns_final(returns)
            annual_return = ep.annual_return(returns)
            annual_volatility = ep.annual_volatility(returns)
            sharpe_ratio = ep.sharpe_ratio(returns, risk_free=config.risk_free_rate)
            max_drawdown = ep.max_drawdown(returns)
            
            # Risk metrics
            sortino_ratio = ep.sortino_ratio(returns, required_return=config.risk_free_rate)
            calmar_ratio = ep.calmar_ratio(returns)
            
            # Handle potential computation errors for advanced metrics
            try:
                omega_ratio = ep.omega_ratio(returns, risk_free=config.risk_free_rate)
            except (ValueError, ZeroDivisionError):
                omega_ratio = np.nan
                
            try:
                tail_ratio = ep.tail_ratio(returns)
            except (ValueError, ZeroDivisionError):
                tail_ratio = np.nan
            
            # VaR calculations
            var_5 = returns.quantile(config.confidence_level)
            cvar_5 = returns[returns <= var_5].mean() if not returns[returns <= var_5].empty else var_5
            
            # Statistical properties
            skewness = returns.skew()
            kurtosis = returns.kurtosis()
            downside_deviation = ep.downside_risk(returns)
            
            # Initialize benchmark metrics as None
            alpha = beta = tracking_error = information_ratio = None
            up_capture = down_capture = None
            
            # Benchmark-relative metrics (if available)
            if benchmark_returns is not None and len(benchmark_returns) == len(returns):
                try:
                    benchmark_returns = benchmark_returns.dropna()
                    benchmark_returns.index = pd.to_datetime(benchmark_returns.index)
                    
                    # Align indices
                    aligned_returns, aligned_benchmark = returns.align(benchmark_returns, join='inner')
                    
                    if len(aligned_returns) > 30:  # Sufficient data for benchmark metrics
                        alpha = ep.alpha(aligned_returns, aligned_benchmark, config.risk_free_rate)
                        beta = ep.beta(aligned_returns, aligned_benchmark)
                        tracking_error = ep.tracking_error(aligned_returns, aligned_benchmark)
                        information_ratio = ep.excess_sharpe(aligned_returns, aligned_benchmark)
                        
                        # Capture ratios
                        try:
                            up_capture = ep.up_capture(aligned_returns, aligned_benchmark)
                            down_capture = ep.down_capture(aligned_returns, aligned_benchmark)
                        except (ValueError, ZeroDivisionError):
                            up_capture = down_capture = np.nan
                            
                except Exception as e:
                    logger.warning(f"Benchmark metrics calculation failed: {e}")
            
            # Create metrics object
            calculation_time = (time.time() - start_time) * 1000  # Convert to milliseconds
            
            metrics = PyFolioMetrics(
                portfolio_id=portfolio_id,
                calculation_time_ms=calculation_time,
                computation_date=datetime.now(),
                total_return=total_return,
                annual_return=annual_return,
                annual_volatility=annual_volatility,
                sharpe_ratio=sharpe_ratio,
                max_drawdown=max_drawdown,
                sortino_ratio=sortino_ratio,
                calmar_ratio=calmar_ratio,
                omega_ratio=omega_ratio,
                tail_ratio=tail_ratio,
                value_at_risk_5=var_5,
                conditional_var_5=cvar_5,
                skewness=skewness,
                kurtosis=kurtosis,
                downside_deviation=downside_deviation,
                alpha=alpha,
                beta=beta,
                tracking_error=tracking_error,
                information_ratio=information_ratio,
                up_capture=up_capture,
                down_capture=down_capture,
                status=AnalyticsStatus.COMPLETED
            )
            
            # Cache the result
            self._cache_analytics(cache_key, metrics)
            
            # Update performance tracking
            self.calculations_performed += 1
            self.total_calculation_time += (calculation_time / 1000)
            
            logger.info(f"PyFolio metrics computed in {calculation_time:.1f}ms for portfolio {portfolio_id}")
            return metrics
            
        except Exception as e:
            logger.error(f"PyFolio metrics calculation failed for portfolio {portfolio_id}: {e}")
            # Return error metrics object
            return PyFolioMetrics(
                portfolio_id=portfolio_id,
                calculation_time_ms=(time.time() - start_time) * 1000,
                computation_date=datetime.now(),
                total_return=np.nan,
                annual_return=np.nan,
                annual_volatility=np.nan,
                sharpe_ratio=np.nan,
                max_drawdown=np.nan,
                sortino_ratio=np.nan,
                calmar_ratio=np.nan,
                omega_ratio=np.nan,
                tail_ratio=np.nan,
                value_at_risk_5=np.nan,
                conditional_var_5=np.nan,
                skewness=np.nan,
                kurtosis=np.nan,
                downside_deviation=np.nan,
                status=AnalyticsStatus.FAILED
            )
    
    async def generate_tear_sheet_data(self, 
                                     portfolio_id: str,
                                     returns: pd.Series,
                                     benchmark_returns: Optional[pd.Series] = None,
                                     positions: Optional[pd.DataFrame] = None,
                                     config: Optional[TearSheetConfig] = None) -> Dict[str, Any]:
        """
        Generate comprehensive tear sheet data for API consumption
        
        Args:
            portfolio_id: Portfolio identifier
            returns: Portfolio returns time series
            benchmark_returns: Optional benchmark returns
            positions: Optional positions DataFrame
            config: Tear sheet configuration
            
        Returns:
            Dictionary with complete tear sheet data
        """
        if not self.available:
            return {"error": "PyFolio not available", "status": "unavailable"}
        
        config = config or self.default_config
        
        try:
            # Get core performance metrics
            metrics = await self.compute_performance_metrics(
                portfolio_id, returns, benchmark_returns, config
            )
            
            # Generate additional tear sheet components
            tear_sheet_data = {
                'metadata': {
                    'portfolio_id': portfolio_id,
                    'generation_time': datetime.now().isoformat(),
                    'data_start': returns.index.min().isoformat() if not returns.empty else None,
                    'data_end': returns.index.max().isoformat() if not returns.empty else None,
                    'total_observations': len(returns)
                },
                'performance_metrics': asdict(metrics),
                'rolling_metrics': await self._compute_rolling_metrics(returns, config),
                'drawdown_analysis': await self._compute_drawdown_analysis(returns),
                'returns_analysis': await self._compute_returns_analysis(returns),
                'risk_analysis': await self._compute_risk_analysis(returns, config)
            }
            
            # Add benchmark comparison if available
            if benchmark_returns is not None:
                tear_sheet_data['benchmark_comparison'] = await self._compute_benchmark_comparison(
                    returns, benchmark_returns
                )
            
            # Add position analysis if available
            if positions is not None and not positions.empty:
                tear_sheet_data['position_analysis'] = await self._compute_position_analysis(
                    positions, returns
                )
            
            return tear_sheet_data
            
        except Exception as e:
            logger.error(f"Tear sheet data generation failed for portfolio {portfolio_id}: {e}")
            return {
                "error": str(e),
                "status": "failed",
                "portfolio_id": portfolio_id,
                "generation_time": datetime.now().isoformat()
            }
    
    async def generate_html_tear_sheet(self, 
                                     portfolio_id: str,
                                     returns: pd.Series,
                                     benchmark_returns: Optional[pd.Series] = None,
                                     positions: Optional[pd.DataFrame] = None,
                                     config: Optional[TearSheetConfig] = None) -> str:
        """
        Generate professional HTML tear sheet for web display
        
        Args:
            portfolio_id: Portfolio identifier
            returns: Portfolio returns time series
            benchmark_returns: Optional benchmark returns
            positions: Optional positions DataFrame
            config: Configuration options
            
        Returns:
            HTML string with professional tear sheet layout
        """
        if not self.available:
            return self._generate_error_html("PyFolio not available")
        
        try:
            # Generate tear sheet data
            data = await self.generate_tear_sheet_data(
                portfolio_id, returns, benchmark_returns, positions, config
            )
            
            if 'error' in data:
                return self._generate_error_html(data['error'])
            
            metrics = data['performance_metrics']
            metadata = data['metadata']
            
            # Generate professional HTML
            html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Portfolio Tear Sheet - {portfolio_id}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f8f9fa;
            color: #333;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            padding: 30px;
        }}
        .header {{
            text-align: center;
            border-bottom: 3px solid #007bff;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }}
        .header h1 {{
            color: #007bff;
            margin: 0 0 10px 0;
            font-size: 2.2em;
        }}
        .metadata {{
            background: #f8f9fa;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 25px;
        }}
        .metrics-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
            margin-bottom: 30px;
        }}
        .metric-section {{
            border: 1px solid #dee2e6;
            border-radius: 6px;
            padding: 20px;
            background: white;
        }}
        .metric-section h3 {{
            color: #495057;
            margin-top: 0;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 8px;
        }}
        .metric-row {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 8px 0;
            border-bottom: 1px solid #f1f3f4;
        }}
        .metric-row:last-child {{
            border-bottom: none;
        }}
        .metric-label {{
            font-weight: 500;
            color: #6c757d;
        }}
        .metric-value {{
            font-weight: 600;
            font-size: 1.1em;
        }}
        .positive {{ color: #28a745; }}
        .negative {{ color: #dc3545; }}
        .neutral {{ color: #007bff; }}
        .warning {{ color: #fd7e14; }}
        .footer {{
            margin-top: 30px;
            padding-top: 20px;
            border-top: 1px solid #dee2e6;
            text-align: center;
            color: #6c757d;
            font-size: 0.9em;
        }}
        .performance-badge {{
            display: inline-block;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 0.85em;
            font-weight: 600;
            margin-left: 8px;
        }}
        .badge-excellent {{ background: #d4edda; color: #155724; }}
        .badge-good {{ background: #d1ecf1; color: #0c5460; }}
        .badge-average {{ background: #fff3cd; color: #856404; }}
        .badge-poor {{ background: #f8d7da; color: #721c24; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Portfolio Performance Analysis</h1>
            <h2>{portfolio_id}</h2>
        </div>
        
        <div class="metadata">
            <strong>Report Generated:</strong> {metadata['generation_time'][:19]} | 
            <strong>Analysis Period:</strong> {metadata['data_start'][:10]} to {metadata['data_end'][:10]} | 
            <strong>Observations:</strong> {metadata['total_observations']} days
        </div>
        
        <div class="metrics-grid">
            <div class="metric-section">
                <h3>üìà Performance Summary</h3>
                <div class="metric-row">
                    <span class="metric-label">Total Return</span>
                    <span class="metric-value {self._get_performance_class(metrics['total_return'])}">{metrics['total_return']:.2%}{self._get_performance_badge(metrics['total_return'], 'return')}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Annualized Return</span>
                    <span class="metric-value {self._get_performance_class(metrics['annual_return'])}">{metrics['annual_return']:.2%}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Annualized Volatility</span>
                    <span class="metric-value neutral">{metrics['annual_volatility']:.2%}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Sharpe Ratio</span>
                    <span class="metric-value {self._get_sharpe_class(metrics['sharpe_ratio'])}">{metrics['sharpe_ratio']:.2f}{self._get_performance_badge(metrics['sharpe_ratio'], 'sharpe')}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Maximum Drawdown</span>
                    <span class="metric-value negative">{metrics['max_drawdown']:.2%}</span>
                </div>
            </div>
            
            <div class="metric-section">
                <h3>‚öñÔ∏è Risk Metrics</h3>
                <div class="metric-row">
                    <span class="metric-label">Sortino Ratio</span>
                    <span class="metric-value {self._get_sharpe_class(metrics['sortino_ratio'])}">{metrics['sortino_ratio']:.2f}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Calmar Ratio</span>
                    <span class="metric-value {self._get_sharpe_class(metrics['calmar_ratio'])}">{metrics['calmar_ratio']:.2f}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Value at Risk (5%)</span>
                    <span class="metric-value negative">{metrics['value_at_risk_5']:.2%}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Conditional VaR (5%)</span>
                    <span class="metric-value negative">{metrics['conditional_var_5']:.2%}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Downside Deviation</span>
                    <span class="metric-value warning">{metrics['downside_deviation']:.2%}</span>
                </div>
            </div>
            
            <div class="metric-section">
                <h3>üìä Statistical Properties</h3>
                <div class="metric-row">
                    <span class="metric-label">Skewness</span>
                    <span class="metric-value {self._get_skew_class(metrics['skewness'])}">{metrics['skewness']:.3f}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Kurtosis</span>
                    <span class="metric-value {self._get_kurtosis_class(metrics['kurtosis'])}">{metrics['kurtosis']:.3f}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Tail Ratio</span>
                    <span class="metric-value neutral">{metrics['tail_ratio']:.3f}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Omega Ratio</span>
                    <span class="metric-value {self._get_omega_class(metrics['omega_ratio'])}">{metrics['omega_ratio']:.3f}</span>
                </div>
            </div>
            
            {self._generate_benchmark_section(metrics) if metrics['alpha'] is not None else ''}
        </div>
        
        <div class="footer">
            <p>Generated by Nautilus Risk Engine | PyFolio Analytics v{self.version}</p>
            <p>Computation completed in {metrics['calculation_time_ms']:.1f}ms | Total calculations: {self.calculations_performed}</p>
        </div>
    </div>
</body>
</html>
            """
            
            return html_content
            
        except Exception as e:
            logger.error(f"HTML tear sheet generation failed for portfolio {portfolio_id}: {e}")
            return self._generate_error_html(f"Generation failed: {str(e)}")
    
    # Helper methods for HTML generation
    def _get_performance_class(self, value: float) -> str:
        """Get CSS class for performance values"""
        if np.isnan(value):
            return "neutral"
        return "positive" if value > 0 else "negative"
    
    def _get_sharpe_class(self, value: float) -> str:
        """Get CSS class for Sharpe-like ratios"""
        if np.isnan(value):
            return "neutral"
        if value > 1.0:
            return "positive"
        elif value > 0.5:
            return "neutral"
        else:
            return "negative"
    
    def _get_skew_class(self, value: float) -> str:
        """Get CSS class for skewness"""
        if np.isnan(value):
            return "neutral"
        if abs(value) < 0.5:
            return "positive"  # Normal distribution
        elif abs(value) < 1.0:
            return "warning"   # Moderate skew
        else:
            return "negative"  # High skew
    
    def _get_kurtosis_class(self, value: float) -> str:
        """Get CSS class for kurtosis"""
        if np.isnan(value):
            return "neutral"
        if abs(value) < 1.0:
            return "positive"  # Normal-ish
        elif abs(value) < 3.0:
            return "warning"   # Moderate excess kurtosis
        else:
            return "negative"  # High excess kurtosis
    
    def _get_omega_class(self, value: float) -> str:
        """Get CSS class for Omega ratio"""
        if np.isnan(value):
            return "neutral"
        if value > 1.2:
            return "positive"
        elif value > 1.0:
            return "neutral"
        else:
            return "negative"
    
    def _get_performance_badge(self, value: float, metric_type: str) -> str:
        """Get performance badge HTML"""
        if np.isnan(value):
            return ""
        
        if metric_type == 'return':
            if value > 0.15:  # >15% return
                return '<span class="performance-badge badge-excellent">Excellent</span>'
            elif value > 0.08:  # >8% return
                return '<span class="performance-badge badge-good">Good</span>'
            elif value > 0.02:  # >2% return
                return '<span class="performance-badge badge-average">Average</span>'
            else:
                return '<span class="performance-badge badge-poor">Poor</span>'
        
        elif metric_type == 'sharpe':
            if value > 1.5:
                return '<span class="performance-badge badge-excellent">Excellent</span>'
            elif value > 1.0:
                return '<span class="performance-badge badge-good">Good</span>'
            elif value > 0.5:
                return '<span class="performance-badge badge-average">Average</span>'
            else:
                return '<span class="performance-badge badge-poor">Poor</span>'
        
        return ""
    
    def _generate_benchmark_section(self, metrics: dict) -> str:
        """Generate benchmark comparison section"""
        return f"""
            <div class="metric-section">
                <h3>üìä vs. Benchmark</h3>
                <div class="metric-row">
                    <span class="metric-label">Alpha</span>
                    <span class="metric-value {self._get_performance_class(metrics['alpha'])}">{metrics['alpha']:.4f}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Beta</span>
                    <span class="metric-value neutral">{metrics['beta']:.3f}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Tracking Error</span>
                    <span class="metric-value warning">{metrics['tracking_error']:.2%}</span>
                </div>
                <div class="metric-row">
                    <span class="metric-label">Information Ratio</span>
                    <span class="metric-value {self._get_sharpe_class(metrics['information_ratio'])}">{metrics['information_ratio']:.3f}</span>
                </div>
            </div>
        """
    
    def _generate_error_html(self, error_message: str) -> str:
        """Generate error HTML page"""
        return f"""
        <html>
        <head><title>Error - Portfolio Analysis</title></head>
        <body style="font-family: Arial, sans-serif; padding: 40px; text-align: center;">
            <h1 style="color: #dc3545;">Portfolio Analysis Error</h1>
            <p style="color: #6c757d; font-size: 1.1em;">{error_message}</p>
            <p style="margin-top: 30px; color: #6c757d;">
                Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            </p>
        </body>
        </html>
        """
    
    # Cache management methods
    def _generate_cache_key(self, portfolio_id: str, returns: pd.Series, 
                           benchmark_returns: Optional[pd.Series] = None) -> str:
        """Generate cache key for analytics"""
        returns_hash = hash(tuple(returns.values)) if not returns.empty else 0
        benchmark_hash = hash(tuple(benchmark_returns.values)) if benchmark_returns is not None else 0
        return f"{portfolio_id}_{returns_hash}_{benchmark_hash}_{len(returns)}"
    
    def _get_cached_analytics(self, cache_key: str) -> Optional[PyFolioMetrics]:
        """Get cached analytics if valid"""
        if cache_key in self.analytics_cache:
            metrics, cache_time = self.analytics_cache[cache_key]
            if datetime.now() - cache_time < self.cache_ttl:
                return metrics
            else:
                # Remove expired cache entry
                del self.analytics_cache[cache_key]
        return None
    
    def _cache_analytics(self, cache_key: str, metrics: PyFolioMetrics) -> None:
        """Cache analytics results"""
        self.analytics_cache[cache_key] = (metrics, datetime.now())
        
        # Clean old cache entries (keep only 100 most recent)
        if len(self.analytics_cache) > 100:
            oldest_keys = sorted(self.analytics_cache.keys(), 
                               key=lambda k: self.analytics_cache[k][1])[:20]
            for key in oldest_keys:
                del self.analytics_cache[key]
    
    # Additional analysis methods
    async def _compute_rolling_metrics(self, returns: pd.Series, 
                                     config: TearSheetConfig) -> Dict[str, Any]:
        """Compute rolling window metrics"""
        try:
            window = config.rolling_window
            if len(returns) < window:
                return {"error": "Insufficient data for rolling metrics"}
            
            rolling_sharpe = returns.rolling(window=window).apply(
                lambda x: ep.sharpe_ratio(x, risk_free=config.risk_free_rate), raw=False
            ).dropna()
            
            rolling_vol = returns.rolling(window=window).std() * np.sqrt(252)
            
            return {
                "rolling_sharpe_6m": rolling_sharpe.to_dict(),
                "rolling_volatility_6m": rolling_vol.to_dict(),
                "window_size": window
            }
        except Exception as e:
            logger.warning(f"Rolling metrics calculation failed: {e}")
            return {"error": str(e)}
    
    async def _compute_drawdown_analysis(self, returns: pd.Series) -> Dict[str, Any]:
        """Compute comprehensive drawdown analysis"""
        try:
            cum_returns = (1 + returns).cumprod()
            rolling_max = cum_returns.expanding().max()
            drawdowns = (cum_returns - rolling_max) / rolling_max
            
            return {
                "drawdown_series": drawdowns.to_dict(),
                "max_drawdown": drawdowns.min(),
                "max_drawdown_date": drawdowns.idxmin().isoformat() if not drawdowns.empty else None,
                "average_drawdown": drawdowns[drawdowns < 0].mean() if any(drawdowns < 0) else 0,
                "drawdown_recovery_time": self._calculate_recovery_time(drawdowns)
            }
        except Exception as e:
            logger.warning(f"Drawdown analysis failed: {e}")
            return {"error": str(e)}
    
    async def _compute_returns_analysis(self, returns: pd.Series) -> Dict[str, Any]:
        """Compute returns distribution analysis"""
        try:
            monthly_returns = ep.aggregate_returns(returns, 'monthly')
            yearly_returns = ep.aggregate_returns(returns, 'yearly')
            
            return {
                "daily_stats": {
                    "mean": returns.mean(),
                    "std": returns.std(),
                    "min": returns.min(),
                    "max": returns.max(),
                    "positive_days": (returns > 0).sum(),
                    "negative_days": (returns < 0).sum()
                },
                "monthly_returns": monthly_returns.to_dict(),
                "yearly_returns": yearly_returns.to_dict(),
                "best_month": monthly_returns.max() if not monthly_returns.empty else None,
                "worst_month": monthly_returns.min() if not monthly_returns.empty else None,
                "best_year": yearly_returns.max() if not yearly_returns.empty else None,
                "worst_year": yearly_returns.min() if not yearly_returns.empty else None
            }
        except Exception as e:
            logger.warning(f"Returns analysis failed: {e}")
            return {"error": str(e)}
    
    async def _compute_risk_analysis(self, returns: pd.Series, 
                                   config: TearSheetConfig) -> Dict[str, Any]:
        """Compute comprehensive risk analysis"""
        try:
            # Multiple VaR levels
            var_levels = [0.01, 0.05, 0.10]
            var_results = {}
            cvar_results = {}
            
            for level in var_levels:
                var_val = returns.quantile(level)
                cvar_val = returns[returns <= var_val].mean() if not returns[returns <= var_val].empty else var_val
                var_results[f"var_{int(level*100)}"] = var_val
                cvar_results[f"cvar_{int(level*100)}"] = cvar_val
            
            return {
                "value_at_risk": var_results,
                "conditional_var": cvar_results,
                "worst_day": returns.min(),
                "best_day": returns.max(),
                "worst_day_date": returns.idxmin().isoformat() if not returns.empty else None,
                "best_day_date": returns.idxmax().isoformat() if not returns.empty else None,
                "volatility_percentiles": {
                    "p25": returns.quantile(0.25),
                    "p50": returns.quantile(0.50),
                    "p75": returns.quantile(0.75),
                    "p95": returns.quantile(0.95),
                    "p99": returns.quantile(0.99)
                }
            }
        except Exception as e:
            logger.warning(f"Risk analysis failed: {e}")
            return {"error": str(e)}
    
    async def _compute_benchmark_comparison(self, returns: pd.Series, 
                                          benchmark_returns: pd.Series) -> Dict[str, Any]:
        """Compute detailed benchmark comparison"""
        try:
            aligned_returns, aligned_benchmark = returns.align(benchmark_returns, join='inner')
            
            if len(aligned_returns) < 30:
                return {"error": "Insufficient aligned data for benchmark comparison"}
            
            excess_returns = aligned_returns - aligned_benchmark
            cum_portfolio = (1 + aligned_returns).cumprod()
            cum_benchmark = (1 + aligned_benchmark).cumprod()
            
            return {
                "excess_returns": excess_returns.to_dict(),
                "cumulative_returns_portfolio": cum_portfolio.to_dict(),
                "cumulative_returns_benchmark": cum_benchmark.to_dict(),
                "periods_outperformed": (excess_returns > 0).sum(),
                "periods_underperformed": (excess_returns < 0).sum(),
                "average_excess_return": excess_returns.mean(),
                "excess_return_volatility": excess_returns.std(),
                "hit_ratio": (excess_returns > 0).mean()
            }
        except Exception as e:
            logger.warning(f"Benchmark comparison failed: {e}")
            return {"error": str(e)}
    
    async def _compute_position_analysis(self, positions: pd.DataFrame, 
                                       returns: pd.Series) -> Dict[str, Any]:
        """Compute position-based analysis (if position data available)"""
        try:
            # This would require position data structure
            # Placeholder implementation
            return {
                "total_positions": len(positions),
                "position_concentration": "analysis_placeholder",
                "sector_allocation": "analysis_placeholder",
                "note": "Position analysis requires specific data structure"
            }
        except Exception as e:
            logger.warning(f"Position analysis failed: {e}")
            return {"error": str(e)}
    
    def _calculate_recovery_time(self, drawdowns: pd.Series) -> int:
        """Calculate average drawdown recovery time in days"""
        try:
            recovery_times = []
            in_drawdown = False
            drawdown_start = None
            
            for date, dd in drawdowns.items():
                if dd < -0.01 and not in_drawdown:  # Start of drawdown (>1%)
                    in_drawdown = True
                    drawdown_start = date
                elif dd >= -0.01 and in_drawdown:  # End of drawdown
                    in_drawdown = False
                    if drawdown_start:
                        recovery_time = (date - drawdown_start).days
                        recovery_times.append(recovery_time)
            
            return int(np.mean(recovery_times)) if recovery_times else 0
        except:
            return 0
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get comprehensive performance statistics for the analytics engine"""
        avg_time = self.total_calculation_time / max(1, self.calculations_performed)
        cache_hit_rate = self.cache_hits / max(1, self.cache_hits + self.cache_misses)
        
        return {
            'pyfolio_available': self.available,
            'pyfolio_version': self.version,
            'calculations_performed': self.calculations_performed,
            'average_calculation_time_ms': avg_time * 1000,
            'total_calculation_time_seconds': self.total_calculation_time,
            'cache_statistics': {
                'cache_hits': self.cache_hits,
                'cache_misses': self.cache_misses,
                'cache_hit_rate': cache_hit_rate,
                'cached_entries': len(self.analytics_cache)
            },
            'performance_metrics': {
                'meets_200ms_target': avg_time * 1000 < 200,
                'fastest_calculation_ms': min(200, avg_time * 1000),  # Placeholder
                'cache_ttl_minutes': self.cache_ttl.total_seconds() / 60
            }
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """Comprehensive health check for PyFolio integration"""
        health_status = {
            'status': 'healthy' if self.available else 'unavailable',
            'pyfolio_available': self.available,
            'version': self.version,
            'last_check': datetime.now().isoformat()
        }
        
        if self.available:
            try:
                # Quick functionality test
                test_returns = pd.Series([0.01, -0.005, 0.02, 0.005, -0.01])
                test_metrics = await self.compute_performance_metrics('health_check', test_returns)
                health_status['functionality_test'] = 'passed'
                health_status['test_calculation_time_ms'] = test_metrics.calculation_time_ms
            except Exception as e:
                health_status['functionality_test'] = 'failed'
                health_status['test_error'] = str(e)
        
        health_status.update(self.get_performance_stats())
        return health_status


# Utility function for external use
async def quick_analytics(portfolio_id: str, returns_data: List[float], 
                         benchmark_data: Optional[List[float]] = None) -> Dict[str, Any]:
    """
    Quick analytics function for external integrations
    
    Args:
        portfolio_id: Portfolio identifier
        returns_data: List of daily returns
        benchmark_data: Optional benchmark returns
        
    Returns:
        Dictionary with analytics results
    """
    analytics = PyFolioAnalytics()
    
    try:
        returns = pd.Series(returns_data)
        benchmark = pd.Series(benchmark_data) if benchmark_data else None
        
        metrics = await analytics.compute_performance_metrics(
            portfolio_id, returns, benchmark
        )
        
        return asdict(metrics)
        
    except Exception as e:
        return {
            'error': str(e),
            'status': 'failed',
            'portfolio_id': portfolio_id
        }