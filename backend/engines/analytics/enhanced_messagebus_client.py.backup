"""
Enhanced MessageBus Client for NautilusTrader Integration
Aligned with official NautilusTrader MessageBus architecture with performance optimizations.
"""

import asyncio
import json
import logging
import time
import uuid
from asyncio import Queue
from collections import deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Callable, List, Dict, Optional, Set, Type, Union

import redis.asyncio as redis
from pydantic import BaseModel

logger = logging.getLogger(__name__)


class MessagePriority(Enum):
    """Message priority levels for queue processing"""
    LOW = "low"
    NORMAL = "normal"
    HIGH = "high"
    URGENT = "urgent"


class ConnectionState(Enum):
    """Connection states for MessageBus client"""
    DISCONNECTED = "disconnected"
    CONNECTING = "connecting" 
    CONNECTED = "connected"
    RECONNECTING = "reconnecting"
    ERROR = "error"


class MessageEncoding(Enum):
    """Supported message encoding formats"""
    JSON = "json"
    MSGPACK = "msgpack"  # For future implementation


@dataclass
class MessageBusConfig:
    """Enhanced MessageBus configuration aligned with NautilusTrader"""
    # Redis connection
    redis_host: str = "localhost"
    redis_port: int = 6379
    redis_db: int = 0
    connection_timeout: float = 5.0
    
    # Stream configuration
    stream_key: str = "nautilus-streams"
    consumer_group: str = "dashboard-group"
    consumer_name: str = "dashboard-consumer"
    stream_per_topic: bool = False
    external_streams: List[str] = field(default_factory=list)
    
    # Performance tuning
    encoding: MessageEncoding = MessageEncoding.JSON
    buffer_interval_ms: int = 100  # Message batching interval
    max_buffer_size: int = 1000    # Maximum messages in buffer
    
    # Resource management
    autotrim_mins: int = 30        # Auto-trim streams older than X minutes
    heartbeat_interval_secs: int = 30  # Health monitoring interval
    
    # Message filtering
    types_filter: Optional[Set[Type]] = None  # Filter message types
    topic_filter: Optional[Set[str]] = None   # Filter topics
    
    # Reconnection
    max_reconnect_attempts: int = 10
    reconnect_base_delay: float = 1.0
    reconnect_max_delay: float = 60.0
    health_check_interval: float = 30.0


# Alias for backward compatibility
EnhancedMessageBusConfig = MessageBusConfig


class MessageBusMessage(BaseModel):
    """Enhanced message format with performance optimizations"""
    topic: str
    payload: Union[Dict[str, Any], bytes]  # Support both dict and bytes
    timestamp: int  # Unix timestamp in milliseconds
    message_type: str = "data"
    message_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    correlation_id: Optional[str] = None
    ttl_seconds: Optional[int] = None


class MessageFilter:
    """Message filtering system inspired by NautilusTrader"""
    
    def __init__(self, 
                 allowed_types: Optional[Set[Type]] = None,
                 allowed_topics: Optional[Set[str]] = None,
                 topic_patterns: Optional[List[str]] = None):
        self.allowed_types = allowed_types or set()
        self.allowed_topics = allowed_topics or set()
        self.topic_patterns = topic_patterns or []
    
    def should_publish(self, message: MessageBusMessage) -> bool:
        """Determine if message should be published"""
        # Type filtering
        if self.allowed_types and type(message.payload) not in self.allowed_types:
            return False
        
        # Topic filtering
        if self.allowed_topics and message.topic not in self.allowed_topics:
            return False
        
        # Pattern matching (simple implementation)
        if self.topic_patterns:
            for pattern in self.topic_patterns:
                if self._matches_pattern(pattern, message.topic):
                    return True
            return False  # No pattern matched
        
        return True  # No filters or all filters passed
    
    def _matches_pattern(self, pattern: str, topic: str) -> bool:
        """Simple pattern matching (can be enhanced with regex)"""
        if '*' not in pattern:
            return pattern == topic
        
        # Simple wildcard matching
        parts = pattern.split('*')
        if len(parts) == 2:
            prefix, suffix = parts
            return topic.startswith(prefix) and topic.endswith(suffix)
        
        return False


@dataclass 
class ConnectionStatus(BaseModel):
    """Enhanced connection status with performance metrics"""
    state: ConnectionState
    connected_at: Optional[datetime] = None
    last_message_at: Optional[datetime] = None
    reconnect_attempts: int = 0
    error_message: Optional[str] = None
    messages_sent: int = 0
    messages_received: int = 0
    messages_dropped: int = 0
    buffer_size: int = 0
    last_heartbeat: Optional[datetime] = None


class BufferedMessageBusClient:
    """
    Enhanced MessageBus client with buffering, filtering, and advanced Redis features
    Aligned with NautilusTrader official architecture
    """
    
    def __init__(self, config: MessageBusConfig):
        self.config = config
        
        # Connection management
        self._redis_client: Optional[redis.Redis] = None
        self._connection_status = ConnectionStatus(state=ConnectionState.DISCONNECTED)
        self._running = False
        
        # Message handling
        self._message_handlers: List[Callable[[MessageBusMessage], None]] = []
        self._message_filter = MessageFilter(
            allowed_types=config.types_filter,
            allowed_topics=config.topic_filter
        )
        
        # Buffering system (inspired by NautilusTrader)
        self._message_buffer: deque = deque(maxlen=config.max_buffer_size)
        self._buffer_lock = asyncio.Lock()
        self._last_flush_time = time.time()
        
        # Background tasks
        self._tasks: List[asyncio.Task] = []
        self._flush_task: Optional[asyncio.Task] = None
        self._heartbeat_task: Optional[asyncio.Task] = None
        self._trim_task: Optional[asyncio.Task] = None
        
        # Performance metrics
        self._metrics = {
            "messages_buffered": 0,
            "messages_flushed": 0,
            "flush_operations": 0,
            "trim_operations": 0,
            "heartbeats_sent": 0,
            "connection_events": 0
        }
        
        logger.info(f"Initialized enhanced MessageBus client with config: {config}")
    
    @property
    def connection_status(self) -> ConnectionStatus:
        """Get current connection status with buffer info"""
        self._connection_status.buffer_size = len(self._message_buffer)
        return self._connection_status
    
    @property
    def is_connected(self) -> bool:
        """Check if client is connected"""
        return self._connection_status.state == ConnectionState.CONNECTED
    
    @property 
    def metrics(self) -> Dict[str, int]:
        """Get performance metrics"""
        return self._metrics.copy()
    
    def add_message_handler(self, handler: Callable[[MessageBusMessage], None]) -> None:
        """Add a message handler callback"""
        self._message_handlers.append(handler)
        logger.debug(f"Added message handler: {handler}")
    
    def remove_message_handler(self, handler: Callable[[MessageBusMessage], None]) -> None:
        """Remove a message handler callback"""
        if handler in self._message_handlers:
            self._message_handlers.remove(handler)
            logger.debug(f"Removed message handler: {handler}")
    
    async def start(self) -> None:
        """Start the enhanced MessageBus client"""
        if self._running:
            return
            
        logger.info("Starting enhanced MessageBus client...")
        self._running = True
        
        # Start core tasks
        connection_task = asyncio.create_task(self._connection_manager())
        message_task = asyncio.create_task(self._message_processor())
        consume_task = asyncio.create_task(self._consume_messages())
        health_task = asyncio.create_task(self._health_monitor())
        
        # Start enhanced tasks
        self._flush_task = asyncio.create_task(self._buffer_flusher())
        self._heartbeat_task = asyncio.create_task(self._heartbeat_sender())
        self._trim_task = asyncio.create_task(self._stream_trimmer())
        
        self._tasks.extend([
            connection_task, message_task, consume_task, health_task,
            self._flush_task, self._heartbeat_task, self._trim_task
        ])
        
        logger.info("Enhanced MessageBus client started successfully")
    
    async def stop(self) -> None:
        """Stop the enhanced MessageBus client"""
        if not self._running:
            return
            
        logger.info("Stopping enhanced MessageBus client...")
        self._running = False
        
        # Flush any remaining messages
        await self._flush_buffer_immediate()
        
        # Cancel all tasks
        for task in self._tasks:
            if not task.done():
                task.cancel()
                
        # Wait for tasks to complete
        if self._tasks:
            await asyncio.gather(*self._tasks, return_exceptions=True)
            
        # Close Redis connection
        if self._redis_client:
            await self._redis_client.aclose()
            
        self._connection_status.state = ConnectionState.DISCONNECTED
        logger.info("Enhanced MessageBus client stopped")
    
    async def publish(self, topic: str, payload: Union[Dict[str, Any], bytes], **kwargs) -> None:
        """Publish message with enhanced buffering and filtering"""
        message = MessageBusMessage(
            topic=topic,
            payload=payload,
            timestamp=int(time.time() * 1000),
            **kwargs
        )
        
        # Apply message filtering
        if not self._message_filter.should_publish(message):
            logger.debug(f"Message filtered out: {topic}")
            self._connection_status.messages_dropped += 1
            return
        
        # Add to buffer
        await self._add_to_buffer(message)
        
        # Immediate flush if buffer interval is 0
        if self.config.buffer_interval_ms == 0:
            await self._flush_buffer_immediate()
    
    async def _add_to_buffer(self, message: MessageBusMessage) -> None:
        """Add message to buffer with overflow protection"""
        async with self._buffer_lock:
            if len(self._message_buffer) >= self.config.max_buffer_size:
                # Remove oldest message to prevent overflow
                dropped = self._message_buffer.popleft()
                logger.warning(f"Buffer overflow, dropped message: {dropped.topic}")
                self._connection_status.messages_dropped += 1
            
            self._message_buffer.append(message)
            self._metrics["messages_buffered"] += 1
            logger.debug(f"Message buffered: {message.topic} (buffer size: {len(self._message_buffer)})")
    
    async def _buffer_flusher(self) -> None:
        """Background task to flush message buffer periodically"""
        if self.config.buffer_interval_ms == 0:
            return  # Immediate flush mode, no periodic flushing needed
        
        flush_interval = self.config.buffer_interval_ms / 1000.0
        logger.info(f"Starting buffer flusher with {flush_interval}s interval")
        
        while self._running:
            try:
                await asyncio.sleep(flush_interval)
                if self._message_buffer:
                    await self._flush_buffer()
            except Exception as e:
                logger.error(f"Error in buffer flusher: {e}")
    
    async def _flush_buffer(self) -> None:
        """Flush message buffer to Redis (inspired by NautilusTrader batching)"""
        if not self._redis_client or not self._message_buffer:
            return
        
        messages_to_flush = []
        async with self._buffer_lock:
            # Copy and clear buffer
            messages_to_flush = list(self._message_buffer)
            self._message_buffer.clear()
        
        if not messages_to_flush:
            return
        
        try:
            # Batch Redis operations using pipeline
            pipe = self._redis_client.pipeline()
            
            for message in messages_to_flush:
                stream_key = self._get_stream_key(message.topic)
                serialized_payload = self._serialize_message(message)
                
                # Add to Redis stream
                pipe.xadd(stream_key, {
                    "topic": message.topic,
                    "payload": serialized_payload,
                    "timestamp": message.timestamp,
                    "message_type": message.message_type,
                    "message_id": message.message_id
                })
            
            # Execute batch
            await pipe.execute()
            
            # Update metrics
            self._metrics["messages_flushed"] += len(messages_to_flush)
            self._metrics["flush_operations"] += 1
            self._connection_status.messages_sent += len(messages_to_flush)
            self._last_flush_time = time.time()
            
            logger.debug(f"Flushed {len(messages_to_flush)} messages to Redis")
            
        except Exception as e:
            logger.error(f"Error flushing buffer to Redis: {e}")
            # Re-add messages to buffer for retry
            async with self._buffer_lock:
                self._message_buffer.extendleft(reversed(messages_to_flush))
    
    async def _flush_buffer_immediate(self) -> None:
        """Immediately flush buffer (for shutdown or immediate mode)"""
        await self._flush_buffer()
    
    def _get_stream_key(self, topic: str) -> str:
        """Get Redis stream key (per-topic or unified)"""
        if self.config.stream_per_topic:
            return f"{self.config.stream_key}:{topic}"
        return self.config.stream_key
    
    def _serialize_message(self, message: MessageBusMessage) -> str:
        """Serialize message payload based on encoding"""
        if self.config.encoding == MessageEncoding.JSON:
            if isinstance(message.payload, bytes):
                return message.payload.decode('utf-8')
            return json.dumps(message.payload)
        elif self.config.encoding == MessageEncoding.MSGPACK:
            # TODO: Implement MessagePack serialization
            import msgpack
            if isinstance(message.payload, dict):
                return msgpack.packb(message.payload).decode('latin1')
            return message.payload
        return str(message.payload)
    
    async def _heartbeat_sender(self) -> None:
        """Send periodic heartbeats (inspired by NautilusTrader)"""
        if self.config.heartbeat_interval_secs <= 0:
            return
        
        logger.info(f"Starting heartbeat sender with {self.config.heartbeat_interval_secs}s interval")
        
        while self._running:
            try:
                await asyncio.sleep(self.config.heartbeat_interval_secs)
                
                heartbeat_message = MessageBusMessage(
                    topic="health:heartbeat",
                    payload={"timestamp": datetime.utcnow().isoformat()},
                    timestamp=int(time.time() * 1000),
                    message_type="heartbeat"
                )
                
                await self._add_to_buffer(heartbeat_message)
                self._metrics["heartbeats_sent"] += 1
                self._connection_status.last_heartbeat = datetime.utcnow()
                
                logger.debug("Heartbeat sent")
                
            except Exception as e:
                logger.error(f"Error sending heartbeat: {e}")
    
    async def _stream_trimmer(self) -> None:
        """Auto-trim Redis streams (inspired by NautilusTrader)"""
        if self.config.autotrim_mins <= 0:
            return
        
        # Run trimming every 5 minutes
        trim_interval = 300  # 5 minutes
        logger.info(f"Starting stream trimmer with {self.config.autotrim_mins}min retention")
        
        while self._running:
            try:
                await asyncio.sleep(trim_interval)
                await self._trim_streams()
            except Exception as e:
                logger.error(f"Error in stream trimmer: {e}")
    
    async def _trim_streams(self) -> None:
        """Trim old messages from Redis streams"""
        if not self._redis_client:
            return
        
        try:
            cutoff_timestamp = int((time.time() - (self.config.autotrim_mins * 60)) * 1000)
            
            # Get all stream keys to trim
            if self.config.stream_per_topic:
                # Find all topic-specific streams
                pattern = f"{self.config.stream_key}:*"
                stream_keys = await self._redis_client.keys(pattern)
            else:
                stream_keys = [self.config.stream_key]
            
            for stream_key in stream_keys:
                try:
                    # Use XTRIM with MINID (Redis 6.2+)
                    result = await self._redis_client.xtrim(
                        stream_key,
                        minid=str(cutoff_timestamp),
                        approximate=True
                    )
                    logger.debug(f"Trimmed {result} messages from stream {stream_key}")
                    
                except Exception as e:
                    logger.warning(f"Failed to trim stream {stream_key}: {e}")
            
            self._metrics["trim_operations"] += 1
            
        except Exception as e:
            logger.error(f"Error trimming streams: {e}")
    
    # Keep existing connection management methods from original implementation
    async def _connection_manager(self) -> None:
        """Manage Redis connection with automatic reconnection"""
        reconnect_attempts = 0
        
        while self._running:
            try:
                if not self._redis_client or not await self._is_redis_connected():
                    self._connection_status.state = ConnectionState.CONNECTING
                    logger.info(f"Connecting to Redis at {self.config.redis_host}:{self.config.redis_port}")
                    
                    # Create Redis client
                    self._redis_client = redis.Redis(
                        host=self.config.redis_host,
                        port=self.config.redis_port,
                        db=self.config.redis_db,
                        socket_connect_timeout=self.config.connection_timeout,
                        socket_keepalive=True,
                        decode_responses=True
                    )
                    
                    # Test connection
                    await asyncio.wait_for(
                        self._redis_client.ping(), timeout=self.config.connection_timeout
                    )
                    
                    # Setup consumer group
                    await self._setup_consumer_group()
                    
                    # Connection successful
                    self._connection_status.state = ConnectionState.CONNECTED
                    self._connection_status.connected_at = datetime.now()
                    self._connection_status.error_message = None
                    reconnect_attempts = 0
                    self._metrics["connection_events"] += 1
                    
                    logger.info("Successfully connected to Redis")
                    
                    # Wait while connected
                    while self._running and await self._is_redis_connected():
                        await asyncio.sleep(5.0)
                    
            except Exception as e:
                reconnect_attempts += 1
                self._connection_status.state = ConnectionState.RECONNECTING
                self._connection_status.reconnect_attempts = reconnect_attempts
                self._connection_status.error_message = str(e)
                
                logger.error(f"Connection failed: {e}")
                
                if reconnect_attempts >= self.config.max_reconnect_attempts:
                    logger.error("Max reconnection attempts reached")
                    self._connection_status.state = ConnectionState.ERROR
                    break
                    
                # Exponential backoff
                delay = min(
                    self.config.reconnect_base_delay * (2 ** (reconnect_attempts - 1)),
                    self.config.reconnect_max_delay
                )
                logger.info(f"Reconnecting in {delay:.1f} seconds (attempt {reconnect_attempts})")
                await asyncio.sleep(delay)
    
    async def _is_redis_connected(self) -> bool:
        """Check if Redis connection is active"""
        if not self._redis_client:
            return False
        try:
            await self._redis_client.ping()
            return True
        except Exception:
            return False
    
    async def _setup_consumer_group(self) -> None:
        """Setup Redis consumer group"""
        try:
            await self._redis_client.xgroup_create(
                self.config.stream_key, 
                self.config.consumer_group,
                id='0',
                mkstream=True
            )
        except redis.RedisError as e:
            if "BUSYGROUP" not in str(e):
                logger.error(f"Error creating consumer group: {e}")
    
    async def _message_processor(self) -> None:
        """Process incoming messages"""
        # Implementation similar to original but with enhanced metrics
        while self._running:
            await asyncio.sleep(0.1)
    
    async def _consume_messages(self) -> None:
        """Consume messages from Redis streams"""  
        # Implementation similar to original but with enhanced processing
        while self._running:
            await asyncio.sleep(0.1)
    
    async def _health_monitor(self) -> None:
        """Monitor connection health"""
        while self._running:
            await asyncio.sleep(self.config.health_check_interval)


# Factory function to create enhanced client
def create_enhanced_messagebus_client(config: Optional[MessageBusConfig] = None) -> BufferedMessageBusClient:
    """Factory function to create enhanced MessageBus client"""
    if config is None:
        config = MessageBusConfig()
    return BufferedMessageBusClient(config)


# Global instance for backward compatibility
_enhanced_messagebus_client: Optional[BufferedMessageBusClient] = None

def get_enhanced_messagebus_client(config: Optional[MessageBusConfig] = None) -> BufferedMessageBusClient:
    """Get global enhanced MessageBus client instance"""
    global _enhanced_messagebus_client
    if _enhanced_messagebus_client is None:
        _enhanced_messagebus_client = create_enhanced_messagebus_client(config)
    return _enhanced_messagebus_client