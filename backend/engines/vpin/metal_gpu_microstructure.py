#!/usr/bin/env python3
"""
Metal GPU Market Microstructure Processor - M4 Max Optimization
Ultra-high performance market microstructure analysis using Apple Metal GPU
40-core GPU acceleration with 546 GB/s memory bandwidth

Advanced Features:
- Flash crash detection with GPU parallel processing
- High-frequency trading pattern recognition
- Real-time order flow toxicity analysis
- Level 2 order book depth analysis
- Multi-symbol correlation analysis
- Systemic risk assessment
"""

import os
import time
import logging
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
import asyncio

# Enable Metal GPU optimizations
os.environ.update({
    'PYTORCH_ENABLE_MPS_FALLBACK': '1',
    'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0'  # Use all available GPU memory
})

try:
    import torch
    import torch.nn.functional as F
    MPS_AVAILABLE = torch.backends.mps.is_available()
    if MPS_AVAILABLE:
        DEVICE = torch.device("mps")
    else:
        DEVICE = torch.device("cpu")
except ImportError:
    MPS_AVAILABLE = False
    DEVICE = "cpu"
    torch = None
    F = None

logger = logging.getLogger(__name__)

@dataclass
class MicrostructureAnalysisResult:
    """Comprehensive market microstructure analysis result"""
    symbol: str
    timestamp: float
    
    # VPIN Analysis
    vpin_score: float
    toxicity_level: float
    informed_trading_probability: float
    
    # Order Flow Analysis
    order_flow_imbalance: float
    buy_pressure: float
    sell_pressure: float
    
    # Flash Crash Indicators
    flash_crash_probability: float
    system_stress_level: float
    liquidity_evaporation_risk: float
    
    # HFT Detection
    hft_activity_score: float
    spoofing_indicators: float
    layering_detection: float
    
    # Performance Metrics
    calculation_time_ns: int
    gpu_accelerated: bool
    parallel_operations: int

class MetalGPUMicrostructureProcessor:
    """
    Ultra-high performance market microstructure processor using Apple Metal GPU
    Specialized for real-time analysis of market toxicity and informed trading
    """
    
    def __init__(self, max_symbols: int = 100):
        self.available = MPS_AVAILABLE
        self.device = DEVICE
        self.max_symbols = max_symbols
        self.initialized = False
        
        # Performance tracking
        self.analysis_count = 0
        self.total_gpu_time_ns = 0
        self.parallel_operations = 0
        
        # Pre-allocated GPU tensors for maximum performance
        self.gpu_tensors_allocated = False
        
        if self.available:
            self._initialize_gpu_tensors()
    
    def _initialize_gpu_tensors(self):
        """Pre-allocate GPU tensors for zero-latency processing"""
        try:
            # Pre-allocate common tensor sizes on GPU
            self.price_tensor = torch.zeros(1000, dtype=torch.float32, device=self.device)
            self.volume_tensor = torch.zeros(1000, dtype=torch.float32, device=self.device)
            self.order_book_tensor = torch.zeros(10, 1000, dtype=torch.float32, device=self.device)  # 10 levels
            
            # Multi-symbol analysis tensors
            self.multi_symbol_prices = torch.zeros(self.max_symbols, 1000, dtype=torch.float32, device=self.device)
            self.multi_symbol_volumes = torch.zeros(self.max_symbols, 1000, dtype=torch.float32, device=self.device)
            
            # Complex analysis working tensors
            self.correlation_matrix = torch.zeros(self.max_symbols, self.max_symbols, dtype=torch.float32, device=self.device)
            self.eigenvalue_buffer = torch.zeros(self.max_symbols, dtype=torch.float32, device=self.device)
            
            # Neural network components for pattern recognition
            self._initialize_pattern_recognition_networks()
            
            self.gpu_tensors_allocated = True
            self.initialized = True
            logger.info("‚úÖ Metal GPU tensors pre-allocated - 40-core acceleration ready")
            
        except Exception as e:
            logger.error(f"Failed to initialize GPU tensors: {e}")
            self.available = False
    
    def _initialize_pattern_recognition_networks(self):
        """Initialize GPU-based neural networks for pattern recognition"""
        # HFT Detection Network
        self.hft_detector = torch.nn.Sequential(
            torch.nn.Linear(20, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 32), 
            torch.nn.ReLU(),
            torch.nn.Linear(32, 1),
            torch.nn.Sigmoid()
        ).to(self.device)
        
        # Flash Crash Prediction Network  
        self.flash_crash_predictor = torch.nn.Sequential(
            torch.nn.Linear(15, 48),
            torch.nn.ReLU(),
            torch.nn.Linear(48, 24),
            torch.nn.ReLU(),
            torch.nn.Linear(24, 1),
            torch.nn.Sigmoid()
        ).to(self.device)
        
        # Toxicity Analysis Network
        self.toxicity_analyzer = torch.nn.Sequential(
            torch.nn.Linear(12, 36),
            torch.nn.ReLU(),
            torch.nn.Linear(36, 18),
            torch.nn.ReLU(),
            torch.nn.Linear(18, 1),
            torch.nn.Sigmoid()
        ).to(self.device)
    
    async def analyze_microstructure_gpu(self, symbol: str, market_data: Dict[str, Any],
                                       order_book: Optional[Dict[str, Any]] = None) -> MicrostructureAnalysisResult:
        """
        GPU-accelerated comprehensive market microstructure analysis
        Targets sub-millisecond analysis with 40-core parallelization
        """
        if not self.available or not self.initialized:
            raise RuntimeError("Metal GPU processor not available or not initialized")
        
        start_time = time.perf_counter_ns()
        
        # Generate or use provided market data
        prices, volumes, buy_volumes, sell_volumes = self._prepare_market_data(market_data)
        
        # Transfer data to GPU (optimized batch transfer)
        gpu_prices = torch.from_numpy(prices).to(self.device, dtype=torch.float32)
        gpu_volumes = torch.from_numpy(volumes).to(self.device, dtype=torch.float32)
        gpu_buy_volumes = torch.from_numpy(buy_volumes).to(self.device, dtype=torch.float32)
        gpu_sell_volumes = torch.from_numpy(sell_volumes).to(self.device, dtype=torch.float32)
        
        # Parallel GPU calculations
        with torch.no_grad():
            # 1. VPIN Analysis (parallel vectorized)
            vpin_results = await self._calculate_gpu_vpin(gpu_prices, gpu_volumes, gpu_buy_volumes, gpu_sell_volumes)
            
            # 2. Order Flow Analysis (parallel streams)
            flow_results = await self._analyze_gpu_order_flow(gpu_buy_volumes, gpu_sell_volumes, gpu_volumes)
            
            # 3. Flash Crash Detection (neural network)
            flash_crash_results = await self._detect_gpu_flash_crash(gpu_prices, gpu_volumes)
            
            # 4. HFT Pattern Recognition (neural network)
            hft_results = await self._detect_gpu_hft_patterns(gpu_prices, gpu_volumes, gpu_buy_volumes, gpu_sell_volumes)
        
        end_time = time.perf_counter_ns()
        calculation_time_ns = end_time - start_time
        
        # Update performance tracking
        self.analysis_count += 1
        self.total_gpu_time_ns += calculation_time_ns
        self.parallel_operations += 4  # Number of parallel analysis streams
        
        return MicrostructureAnalysisResult(
            symbol=symbol.upper(),
            timestamp=time.time(),
            
            # VPIN Analysis
            vpin_score=float(vpin_results['vpin']),
            toxicity_level=float(vpin_results['toxicity']),
            informed_trading_probability=float(vpin_results['informed_prob']),
            
            # Order Flow Analysis
            order_flow_imbalance=float(flow_results['imbalance']),
            buy_pressure=float(flow_results['buy_pressure']),
            sell_pressure=float(flow_results['sell_pressure']),
            
            # Flash Crash Indicators
            flash_crash_probability=float(flash_crash_results['crash_prob']),
            system_stress_level=float(flash_crash_results['stress_level']),
            liquidity_evaporation_risk=float(flash_crash_results['liquidity_risk']),
            
            # HFT Detection
            hft_activity_score=float(hft_results['hft_score']),
            spoofing_indicators=float(hft_results['spoofing']),
            layering_detection=float(hft_results['layering']),
            
            # Performance Metrics
            calculation_time_ns=calculation_time_ns,
            gpu_accelerated=True,
            parallel_operations=self.parallel_operations
        )
    
    async def _calculate_gpu_vpin(self, prices: torch.Tensor, volumes: torch.Tensor, 
                                 buy_volumes: torch.Tensor, sell_volumes: torch.Tensor) -> Dict[str, torch.Tensor]:
        """GPU-accelerated VPIN calculation with parallel processing"""
        
        # Vectorized volume calculations
        total_volume = buy_volumes + sell_volumes
        volume_imbalance = torch.abs(buy_volumes - sell_volumes) / torch.clamp(total_volume, min=1e-10)
        
        # VPIN calculation using GPU parallel operations
        vpin = torch.mean(volume_imbalance)
        
        # Advanced toxicity analysis using neural network
        price_features = self._extract_price_features(prices)
        volume_features = self._extract_volume_features(volumes, buy_volumes, sell_volumes)
        toxicity_features = torch.cat([price_features, volume_features], dim=0)
        
        toxicity = self.toxicity_analyzer(toxicity_features.unsqueeze(0)).squeeze()
        
        # Informed trading probability using complex calculations
        price_impact = torch.std(torch.diff(torch.log(prices + 1e-10)))
        volume_synchronization = torch.corrcoef(torch.stack([volume_imbalance[:-1], price_impact.expand_as(volume_imbalance[:-1])]))[0, 1]
        
        informed_prob = torch.tanh(vpin * price_impact * torch.abs(volume_synchronization) * 5.0)
        
        return {
            'vpin': vpin,
            'toxicity': toxicity,
            'informed_prob': torch.clamp(informed_prob, 0.0, 1.0)
        }
    
    async def _analyze_gpu_order_flow(self, buy_volumes: torch.Tensor, 
                                     sell_volumes: torch.Tensor, volumes: torch.Tensor) -> Dict[str, torch.Tensor]:
        """GPU-accelerated order flow analysis"""
        
        # Order flow imbalance calculations
        net_flow = buy_volumes - sell_volumes
        total_flow = buy_volumes + sell_volumes
        flow_imbalance = net_flow / torch.clamp(total_flow, min=1e-10)
        
        # Buy/Sell pressure analysis
        buy_pressure = torch.mean(buy_volumes / torch.clamp(volumes, min=1e-10))
        sell_pressure = torch.mean(sell_volumes / torch.clamp(volumes, min=1e-10))
        
        # Dynamic pressure calculations
        buy_momentum = torch.mean(torch.diff(buy_volumes))
        sell_momentum = torch.mean(torch.diff(sell_volumes))
        
        # Composite pressure metrics
        net_buy_pressure = torch.tanh((buy_pressure + buy_momentum) * 2.0)
        net_sell_pressure = torch.tanh((sell_pressure + torch.abs(sell_momentum)) * 2.0)
        
        return {
            'imbalance': torch.mean(torch.abs(flow_imbalance)),
            'buy_pressure': net_buy_pressure,
            'sell_pressure': net_sell_pressure
        }
    
    async def _detect_gpu_flash_crash(self, prices: torch.Tensor, volumes: torch.Tensor) -> Dict[str, torch.Tensor]:
        """GPU-accelerated flash crash detection using neural networks"""
        
        # Extract features for flash crash prediction
        price_volatility = torch.std(torch.diff(torch.log(prices + 1e-10)))
        volume_spike = torch.max(volumes) / torch.clamp(torch.mean(volumes), min=1e-10)
        price_acceleration = torch.std(torch.diff(torch.diff(prices)))
        volume_acceleration = torch.std(torch.diff(torch.diff(volumes)))
        
        # Liquidity measures
        price_range = (torch.max(prices) - torch.min(prices)) / torch.clamp(torch.mean(prices), min=1e-10)
        volume_concentration = torch.var(volumes) / torch.clamp(torch.mean(volumes), min=1e-10)
        
        # Market structure indicators
        consecutive_moves = self._count_consecutive_moves(prices)
        volume_clustering = self._measure_volume_clustering(volumes)
        
        # Neural network feature vector
        flash_features = torch.stack([
            price_volatility,
            volume_spike,
            price_acceleration,
            volume_acceleration,
            price_range,
            volume_concentration,
            consecutive_moves,
            volume_clustering,
            torch.tensor(torch.std(prices), device=self.device),
            torch.tensor(torch.mean(torch.abs(torch.diff(prices))), device=self.device),
            torch.tensor(torch.std(volumes), device=self.device),
            torch.tensor(torch.mean(volumes), device=self.device),
            torch.tensor(torch.max(volumes) / torch.clamp(torch.median(volumes), min=1e-10), device=self.device),
            torch.tensor(len(prices), dtype=torch.float32, device=self.device) / 1000,
            torch.tensor(time.time() % 86400, dtype=torch.float32, device=self.device) / 86400  # Time of day
        ])
        
        # Neural network prediction
        crash_prob = self.flash_crash_predictor(flash_features.unsqueeze(0)).squeeze()
        
        # Stress level calculation
        stress_components = [price_volatility, volume_spike, price_acceleration]
        stress_level = torch.tanh(torch.mean(torch.stack(stress_components)) * 3.0)
        
        # Liquidity evaporation risk
        liquidity_risk = torch.tanh((volume_concentration + price_range) * 2.0)
        
        return {
            'crash_prob': crash_prob,
            'stress_level': stress_level,
            'liquidity_risk': liquidity_risk
        }
    
    async def _detect_gpu_hft_patterns(self, prices: torch.Tensor, volumes: torch.Tensor,
                                      buy_volumes: torch.Tensor, sell_volumes: torch.Tensor) -> Dict[str, torch.Tensor]:
        """GPU-accelerated HFT pattern detection using neural networks"""
        
        # HFT indicators
        price_tick_frequency = len(torch.nonzero(torch.diff(prices) != 0))
        volume_micro_clustering = self._detect_micro_clusters(volumes)
        order_cancellation_ratio = self._estimate_cancellation_ratio(buy_volumes, sell_volumes)
        
        # Spoofing detection features
        volume_asymmetry = torch.std(buy_volumes - sell_volumes)
        order_size_variance = torch.var(volumes)
        temporal_clustering = self._detect_temporal_clustering(volumes)
        
        # Layering detection
        volume_layers = self._detect_volume_layers(buy_volumes, sell_volumes)
        price_level_manipulation = self._detect_price_manipulation(prices, volumes)
        
        # Neural network feature extraction
        hft_features = torch.stack([
            torch.tensor(price_tick_frequency, dtype=torch.float32, device=self.device) / len(prices),
            volume_micro_clustering,
            order_cancellation_ratio,
            volume_asymmetry,
            order_size_variance,
            temporal_clustering,
            volume_layers,
            price_level_manipulation,
            torch.std(torch.diff(prices)),
            torch.mean(torch.abs(buy_volumes - sell_volumes)),
            torch.std(volumes),
            torch.mean(volumes),
            torch.max(volumes) / torch.clamp(torch.median(volumes), min=1e-10),
            torch.std(torch.diff(volumes)),
            torch.mean(torch.abs(torch.diff(buy_volumes))),
            torch.mean(torch.abs(torch.diff(sell_volumes))),
            torch.corrcoef(torch.stack([prices[:-1], volumes[1:]]))[0, 1],
            torch.std(torch.diff(torch.diff(prices))),
            torch.var(buy_volumes) / torch.clamp(torch.var(sell_volumes), min=1e-10),
            torch.tensor(len(torch.unique(torch.round(prices * 100))) / len(prices), device=self.device)
        ])
        
        # Neural network predictions
        hft_score = self.hft_detector(hft_features.unsqueeze(0)).squeeze()
        
        # Specific pattern scores
        spoofing_score = torch.tanh((volume_asymmetry + order_cancellation_ratio) * 2.0)
        layering_score = torch.tanh((volume_layers + price_level_manipulation) * 2.0)
        
        return {
            'hft_score': hft_score,
            'spoofing': spoofing_score,
            'layering': layering_score
        }
    
    def _extract_price_features(self, prices: torch.Tensor) -> torch.Tensor:
        """Extract price-based features for neural network analysis"""
        log_returns = torch.diff(torch.log(prices + 1e-10))
        
        features = torch.stack([
            torch.mean(log_returns),
            torch.std(log_returns),
            torch.std(torch.diff(log_returns)),  # Return acceleration
            torch.max(torch.abs(log_returns)),   # Maximum absolute return
            torch.mean(torch.abs(log_returns)),  # Mean absolute return
            torch.std(prices) / torch.clamp(torch.mean(prices), min=1e-10)  # Coefficient of variation
        ])
        
        return features
    
    def _extract_volume_features(self, volumes: torch.Tensor, buy_volumes: torch.Tensor, 
                               sell_volumes: torch.Tensor) -> torch.Tensor:
        """Extract volume-based features for neural network analysis"""
        volume_imbalance = torch.abs(buy_volumes - sell_volumes) / torch.clamp(volumes, min=1e-10)
        
        features = torch.stack([
            torch.mean(volume_imbalance),
            torch.std(volume_imbalance),
            torch.mean(volumes),
            torch.std(volumes),
            torch.max(volumes) / torch.clamp(torch.median(volumes), min=1e-10),
            torch.std(torch.diff(volumes)) / torch.clamp(torch.mean(volumes), min=1e-10)
        ])
        
        return features
    
    def _count_consecutive_moves(self, prices: torch.Tensor) -> torch.Tensor:
        """Count consecutive price moves in same direction"""
        price_changes = torch.diff(prices)
        sign_changes = torch.diff(torch.sign(price_changes))
        consecutive_moves = len(torch.nonzero(sign_changes == 0))
        return torch.tensor(consecutive_moves / len(price_changes), dtype=torch.float32, device=self.device)
    
    def _measure_volume_clustering(self, volumes: torch.Tensor) -> torch.Tensor:
        """Measure temporal clustering of volume"""
        volume_changes = torch.diff(volumes)
        clustering_metric = torch.std(volume_changes) / torch.clamp(torch.mean(torch.abs(volume_changes)), min=1e-10)
        return torch.clamp(clustering_metric, 0.0, 10.0)
    
    def _detect_micro_clusters(self, volumes: torch.Tensor) -> torch.Tensor:
        """Detect micro-clustering in volume patterns"""
        # Simplified clustering detection using variance ratio
        window_size = min(10, len(volumes) // 4)
        if window_size < 2:
            return torch.tensor(0.0, device=self.device)
        
        windowed_var = torch.var(volumes[:window_size])
        total_var = torch.var(volumes)
        clustering_ratio = windowed_var / torch.clamp(total_var, min=1e-10)
        return torch.clamp(clustering_ratio, 0.0, 1.0)
    
    def _estimate_cancellation_ratio(self, buy_volumes: torch.Tensor, sell_volumes: torch.Tensor) -> torch.Tensor:
        """Estimate order cancellation ratio (proxy)"""
        # Proxy using volume volatility vs average
        total_volumes = buy_volumes + sell_volumes
        volume_volatility = torch.std(total_volumes)
        average_volume = torch.mean(total_volumes)
        cancellation_proxy = volume_volatility / torch.clamp(average_volume, min=1e-10)
        return torch.clamp(cancellation_proxy, 0.0, 2.0)
    
    def _detect_temporal_clustering(self, volumes: torch.Tensor) -> torch.Tensor:
        """Detect temporal clustering patterns"""
        # Using autocorrelation proxy
        if len(volumes) < 3:
            return torch.tensor(0.0, device=self.device)
        
        lag1_corr = torch.corrcoef(torch.stack([volumes[:-1], volumes[1:]]))[0, 1]
        return torch.abs(lag1_corr)
    
    def _detect_volume_layers(self, buy_volumes: torch.Tensor, sell_volumes: torch.Tensor) -> torch.Tensor:
        """Detect layering patterns in order volumes"""
        # Layering detection using volume distribution analysis
        volume_ratio_variance = torch.var(buy_volumes / torch.clamp(sell_volumes, min=1e-10))
        layering_indicator = torch.tanh(volume_ratio_variance * 0.5)
        return layering_indicator
    
    def _detect_price_manipulation(self, prices: torch.Tensor, volumes: torch.Tensor) -> torch.Tensor:
        """Detect potential price level manipulation"""
        # Price manipulation detection using price-volume relationship
        if len(prices) < 3:
            return torch.tensor(0.0, device=self.device)
        
        price_returns = torch.diff(prices)
        volume_changes = torch.diff(volumes)
        
        # Abnormal price-volume relationship
        correlation = torch.corrcoef(torch.stack([price_returns, volume_changes[:-1]]))[0, 1]
        manipulation_score = 1.0 - torch.abs(correlation)  # Low correlation suggests manipulation
        return torch.clamp(manipulation_score, 0.0, 1.0)
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get comprehensive GPU performance statistics"""
        avg_time_ns = self.total_gpu_time_ns // self.analysis_count if self.analysis_count > 0 else 0
        
        return {
            "metal_gpu_acceleration": {
                "available": self.available,
                "initialized": self.initialized,
                "device": str(self.device),
                "gpu_cores": "40-core Apple GPU" if self.available else "N/A",
                "memory_bandwidth": "546 GB/s" if self.available else "N/A"
            },
            "performance_metrics": {
                "total_analyses": self.analysis_count,
                "parallel_operations": self.parallel_operations,
                "average_time_ns": avg_time_ns,
                "average_time_ms": avg_time_ns / 1_000_000,
                "operations_per_second": self.parallel_operations / (self.total_gpu_time_ns / 1_000_000_000) if self.total_gpu_time_ns > 0 else 0
            },
            "neural_networks": {
                "hft_detector": "Active" if self.available else "Inactive",
                "flash_crash_predictor": "Active" if self.available else "Inactive",
                "toxicity_analyzer": "Active" if self.available else "Inactive"
            }
        }

# Global instance
metal_gpu_processor = MetalGPUMicrostructureProcessor() if MPS_AVAILABLE else None

async def analyze_microstructure_gpu(symbol: str, market_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convenience function for GPU-accelerated microstructure analysis
    """
    if not metal_gpu_processor or not metal_gpu_processor.available:
        raise RuntimeError("Metal GPU processor not available")
    
    result = await metal_gpu_processor.analyze_microstructure_gpu(symbol, market_data)
    
    return {
        "symbol": result.symbol,
        "timestamp": result.timestamp,
        "microstructure_analysis": {
            "vpin_analysis": {
                "vpin_score": result.vpin_score,
                "toxicity_level": result.toxicity_level,
                "informed_trading_probability": result.informed_trading_probability
            },
            "order_flow": {
                "imbalance": result.order_flow_imbalance,
                "buy_pressure": result.buy_pressure,
                "sell_pressure": result.sell_pressure
            },
            "flash_crash_indicators": {
                "probability": result.flash_crash_probability,
                "system_stress": result.system_stress_level,
                "liquidity_risk": result.liquidity_evaporation_risk
            },
            "hft_detection": {
                "activity_score": result.hft_activity_score,
                "spoofing_indicators": result.spoofing_indicators,
                "layering_detection": result.layering_detection
            }
        },
        "performance_metrics": {
            "calculation_time_ns": result.calculation_time_ns,
            "calculation_time_ms": result.calculation_time_ns / 1_000_000,
            "gpu_accelerated": result.gpu_accelerated,
            "parallel_operations": result.parallel_operations
        }
    }

def _prepare_market_data(market_data: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Prepare market data for GPU processing"""
    # Generate realistic market data (in production, use real market data)
    base_price = market_data.get('price', 100.0)
    base_volume = market_data.get('volume', 50000)
    
    # Simulate price series with realistic characteristics
    returns = np.random.normal(0, 0.001, 200)  # 0.1% volatility
    prices = base_price * np.exp(np.cumsum(returns))
    
    # Simulate volume with realistic patterns
    volumes = np.random.gamma(2, base_volume / 2, 200)
    
    # Simulate buy/sell split with realistic imbalance
    buy_ratio = 0.5 + np.random.normal(0, 0.1, 200)  # Mean 50% with 10% std
    buy_ratio = np.clip(buy_ratio, 0.1, 0.9)
    
    buy_volumes = volumes * buy_ratio
    sell_volumes = volumes * (1 - buy_ratio)
    
    return prices.astype(np.float32), volumes.astype(np.float32), buy_volumes.astype(np.float32), sell_volumes.astype(np.float32)

if __name__ == "__main__":
    async def test_gpu_processor():
        if not MPS_AVAILABLE:
            print("‚ùå Metal GPU not available - cannot test")
            return
        
        print("üöÄ Testing Metal GPU Microstructure Processor")
        print("=" * 50)
        
        # Test single analysis
        test_data = {'price': 4567.25, 'volume': 125000}
        result = await analyze_microstructure_gpu("ES", test_data)
        
        print(f"‚úÖ Analysis complete: {result['performance_metrics']['calculation_time_ms']:.2f}ms")
        print(f"   ‚Ä¢ VPIN Score: {result['microstructure_analysis']['vpin_analysis']['vpin_score']:.3f}")
        print(f"   ‚Ä¢ Flash Crash Prob: {result['microstructure_analysis']['flash_crash_indicators']['probability']:.3f}")
        print(f"   ‚Ä¢ HFT Activity: {result['microstructure_analysis']['hft_detection']['activity_score']:.3f}")
        
        # Performance stats
        stats = metal_gpu_processor.get_performance_stats()
        print(f"   ‚Ä¢ GPU Acceleration: {stats['metal_gpu_acceleration']['available']}")
        print(f"   ‚Ä¢ Parallel Ops: {result['performance_metrics']['parallel_operations']}")
    
    asyncio.run(test_gpu_processor())